{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "train = pd.read_csv('remove_duplicates_transform_label_address_train.csv')\n",
    "test = pd.read_csv('remove_duplicates_transform_label_address_test.csv')\n",
    "sample = pd.read_csv('input/sampleSubmission.csv')\n",
    "train['Zipcode']=train['Zipcode'].astype(str)\n",
    "test['Zipcode']=test['Zipcode'].astype(str)\n",
    "\n",
    "weather = pd.read_csv('input/weather.csv')\n",
    "# spray = pd.read_csv('input/spray.csv')\n",
    "# weather station coordinates\n",
    "station1 = [-87.933 , 41.995]\n",
    "station2 = [-87.752 , 41.786]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_features(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['Date'].apply(lambda x: x.month)\n",
    "    df['DayOfMonth'] = df['Date'].apply(lambda x: x.day)\n",
    "    # df['DayOfWeekName'] = df['Date'].apply(lambda x: x.day_name())\n",
    "    df['DayOfWeek'] = df['Date'].apply(lambda x: x.dayofweek)\n",
    "    df['DayOfYear'] = df['Date'].apply(lambda x: x.dayofyear)\n",
    "#     df['WeekOfYear'] = df['Date'].apply(lambda x: x.weekofyear)\n",
    "#     df['IsLeapYear'] = df['Date'].apply(lambda x: x.is_leap_year)\n",
    "#     df['IsLeapYear'] = df['IsLeapYear'].astype(str)\n",
    "#     df['Quarter'] = df['Date'].apply(lambda x: x.quarter)\n",
    "\n",
    "def transform_df(df):\n",
    "    df_ = df.copy()\n",
    "    df_['Lat_int'] = df_.Latitude.astype(int)\n",
    "    df_['Long_int'] = df_.Longitude.astype(int)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = transform_df(train)\n",
    "test2 = transform_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                object\n",
       "Species             object\n",
       "Block                int64\n",
       "Street              object\n",
       "Trap                object\n",
       "Latitude           float64\n",
       "Longitude          float64\n",
       "AddressAccuracy      int64\n",
       "WnvPresent           int64\n",
       "City                object\n",
       "State               object\n",
       "Zipcode             object\n",
       "Lat_int              int64\n",
       "Long_int             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featurize weather, remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using codesum for this benchmark\n",
    "weather = weather.drop('CodeSum', axis=1)\n",
    "\n",
    "# Split station 1 and 2 and join horizontally\n",
    "weather_stn1 = weather[weather['Station']==1]\n",
    "weather_stn2 = weather[weather['Station']==2]\n",
    "weather_stn1 = weather_stn1.drop('Station', axis=1)\n",
    "weather_stn2 = weather_stn2.drop('Station', axis=1)\n",
    "weather = weather_stn1.merge(weather_stn2, on='Date')\n",
    "\n",
    "# replace some missing values and T with -1\n",
    "weather = weather.replace('M', -1)\n",
    "weather = weather.replace('-', -1)\n",
    "weather = weather.replace('T', -1)\n",
    "weather = weather.replace(' T', -1)\n",
    "weather = weather.replace('  T', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with weather data\n",
    "train3 = train2.merge(weather, on='Date')\n",
    "test3 = test2.merge(weather, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                object\n",
       "Species             object\n",
       "Block                int64\n",
       "Street              object\n",
       "Trap                object\n",
       "Latitude           float64\n",
       "Longitude          float64\n",
       "AddressAccuracy      int64\n",
       "WnvPresent           int64\n",
       "City                object\n",
       "State               object\n",
       "Zipcode             object\n",
       "Lat_int              int64\n",
       "Long_int             int64\n",
       "Tmax_x               int64\n",
       "Tmin_x               int64\n",
       "Tavg_x              object\n",
       "Depart_x            object\n",
       "DewPoint_x           int64\n",
       "WetBulb_x           object\n",
       "Heat_x              object\n",
       "Cool_x              object\n",
       "Sunrise_x           object\n",
       "Sunset_x            object\n",
       "Depth_x             object\n",
       "Water1_x             int64\n",
       "SnowFall_x          object\n",
       "PrecipTotal_x       object\n",
       "StnPressure_x       object\n",
       "SeaLevel_x          object\n",
       "ResultSpeed_x      float64\n",
       "ResultDir_x          int64\n",
       "AvgSpeed_x          object\n",
       "Tmax_y               int64\n",
       "Tmin_y               int64\n",
       "Tavg_y              object\n",
       "Depart_y             int64\n",
       "DewPoint_y           int64\n",
       "WetBulb_y           object\n",
       "Heat_y              object\n",
       "Cool_y              object\n",
       "Sunrise_y            int64\n",
       "Sunset_y             int64\n",
       "Depth_y              int64\n",
       "Water1_y             int64\n",
       "SnowFall_y           int64\n",
       "PrecipTotal_y       object\n",
       "StnPressure_y       object\n",
       "SeaLevel_y          object\n",
       "ResultSpeed_y      float64\n",
       "ResultDir_y          int64\n",
       "AvgSpeed_y          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop address columns\n",
    "train3 = train3.drop(['Date','WnvPresent'],  axis = 1)\n",
    "test3 = test3.drop(['Date','Id'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8475, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_encoder(train, test, cols):\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    for col in cols:\n",
    "        lbl.fit(list(train[col].values) + list(test[col].values))\n",
    "        train[col] = lbl.transform(train[col].values)\n",
    "        test[col] = lbl.transform(test[col].values)\n",
    "def convert_string_to_num(df, str_columns):\n",
    "    for col in str_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "def get_metrics(y_true, y_pred, y_pred_prob):\n",
    "    print(\"Precision: %1.3f\" % precision_score(y_true, y_pred))\n",
    "    print(\"Recall: %1.3f\" % recall_score(y_true, y_pred))\n",
    "    print(\"F1: %1.3f\" % f1_score(y_true, y_pred))\n",
    "    print(\"AUC: %1.3f\" % roc_auc_score(y_true, y_pred))\n",
    "    print(\"AUC with probability: %1.3f\" % roc_auc_score(y_true, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Species', 'Street', 'Trap', 'City', 'State', 'Zipcode', 'Tavg_x',\n",
       "       'Depart_x', 'WetBulb_x', 'Heat_x', 'Cool_x', 'Sunrise_x', 'Sunset_x',\n",
       "       'Depth_x', 'SnowFall_x', 'PrecipTotal_x', 'StnPressure_x', 'SeaLevel_x',\n",
       "       'AvgSpeed_x', 'Tavg_y', 'WetBulb_y', 'Heat_y', 'Cool_y',\n",
       "       'PrecipTotal_y', 'StnPressure_y', 'SeaLevel_y', 'AvgSpeed_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_obj_cols = train3.select_dtypes(include=['object']).columns\n",
    "all_obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_columns = [u'Tavg_x', u'Depart_x', u'WetBulb_x',\n",
    "       u'Heat_x', u'Cool_x', u'Sunrise_x', u'Sunset_x', u'Depth_x',\n",
    "       u'SnowFall_x', u'PrecipTotal_x', u'StnPressure_x', u'SeaLevel_x',\n",
    "       u'AvgSpeed_x', u'Tavg_y', u'WetBulb_y', u'Heat_y', u'Cool_y',\n",
    "       u'PrecipTotal_y', u'StnPressure_y', u'SeaLevel_y', u'AvgSpeed_y']\n",
    "convert_string_to_num(train3,str_columns )\n",
    "convert_string_to_num(test3,str_columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = [u'Species', u'Street', u'Trap','State','City','Zipcode']\n",
    "get_label_encoder(train3, test3, obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# remove missing data\n",
    "train4 = train3.ix[:,(train3 != -1).any(axis=0)]\n",
    "test4 = test3.ix[:,(test3 != -1).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8475, 43)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 43)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train4\n",
    "y = train.WnvPresent.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42) # need to convert to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "learning_rate = [0.08,0.12]\n",
    "max_depth = [6, 8]\n",
    "n_estimators = [150, 200]\n",
    "#min_samples_split = [2,10,50]\n",
    "\n",
    "tuned_parameters = [{'max_depth': max_depth,\n",
    "                     'n_estimators':n_estimators,\n",
    "                     'learning_rate':learning_rate ,\n",
    "                     #'min_samples_split':min_samples_split\n",
    "                    }]\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv with aucroc\n",
    "clf = GridSearchCV(gbt, \n",
    "                   tuned_parameters, \n",
    "                   cv=n_folds, \n",
    "                   refit=True, \n",
    "                   scoring='recall') # what if i set as recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 260 ms, total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [6, 8], 'n_estimators': [150, 200], 'learning_rate': [0.08, 0.12]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='recall', verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.08, 'max_depth': 6, 'n_estimators': 200}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred_prob = clf.predict_proba(X_train)\n",
    "y_test_pred_prob = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print (unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [5619  313]\n"
     ]
    }
   ],
   "source": [
    "count_label(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [5666  266]\n"
     ]
    }
   ],
   "source": [
    "count_label(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [2399  144]\n"
     ]
    }
   ],
   "source": [
    "count_label(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [2479   64]\n"
     ]
    }
   ],
   "source": [
    "count_label(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2399,  144]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.996\n",
      "Recall: 0.847\n",
      "F1: 0.915\n",
      "AUC: 0.923\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (5932, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-4e823cda8ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-52c7a2bb62dc>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(y_true, y_pred, y_pred_prob)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1: %1.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC: %1.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC with probability: %1.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    354\u001b[0m     return _average_binary_score(\n\u001b[1;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 328\u001b[0;31m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \"\"\"\n\u001b[1;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 618\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (5932, 2)"
     ]
    }
   ],
   "source": [
    "get_metrics(y_train, y_train_pred, y_train_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_metrics(y_test, y_test_pred, y_test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction(clf, test, sample):\n",
    "    predictions = clf.predict(test)\n",
    "    sample['WnvPresent'] = predictions \n",
    "    # wait why is it not binary prediction, all zeros????\n",
    "    # why it is still all 0\n",
    "    print (sample.WnvPresent.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    115936\n",
      "1       357\n",
      "Name: WnvPresent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "create_prediction(clf, test4, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('exp4_2.csv', index=False)\n",
    "# change naming system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use different scoring metrics\n",
    "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
    "recall = TP/(TP+FN), in case if predictor doesn't predict positive class - TP is 0 - recall is 0.\n",
    "now you are dividing 0/0.\n",
    "TP is 0 ==> set precision as 0\n",
    "precision = TP / (TP + FP) # no positive at all\n",
    "recall = TP / (TP + FN) # there is always false negative\n",
    "==> no positive precision at all???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scorers can be either be one of the predefined metric strings or a scorer\n",
    "# callable, like the one returned by make_scorer\n",
    "scoring = {'AUC': 'roc_auc', \n",
    "           'f1': 'f1',\n",
    "           'recall':'recall',\n",
    "           'precision':'precision'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/juyang/anaconda2/envs/sparkbeyond/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'max_depth': [3, 6, 8], 'n_estimators': [100, 150, 200], 'learning_rate': [0.03, 0.08, 0.12], 'min_samples_split': [2, 10, 50]}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring={'AUC': 'roc_auc', 'f1': 'f1', 'recall': 'recall', 'precision': 'precision'},\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = GridSearchCV(gbt, \n",
    "                   tuned_parameters, \n",
    "                   cv=n_folds, \n",
    "                   refit=False, \n",
    "                   scoring=scoring)  \n",
    "# what if i set as recall\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_learning_rate', 'param_max_depth', 'param_min_samples_split',\n",
       "       'param_n_estimators', 'params', 'split0_test_AUC', 'split1_test_AUC',\n",
       "       'split2_test_AUC', 'mean_test_AUC', 'std_test_AUC', 'rank_test_AUC',\n",
       "       'split0_train_AUC', 'split1_train_AUC', 'split2_train_AUC',\n",
       "       'mean_train_AUC', 'std_train_AUC', 'split0_test_f1', 'split1_test_f1',\n",
       "       'split2_test_f1', 'mean_test_f1', 'std_test_f1', 'rank_test_f1',\n",
       "       'split0_train_f1', 'split1_train_f1', 'split2_train_f1',\n",
       "       'mean_train_f1', 'std_train_f1', 'split0_test_recall',\n",
       "       'split1_test_recall', 'split2_test_recall', 'mean_test_recall',\n",
       "       'std_test_recall', 'rank_test_recall', 'split0_train_recall',\n",
       "       'split1_train_recall', 'split2_train_recall', 'mean_train_recall',\n",
       "       'std_train_recall', 'split0_test_precision', 'split1_test_precision',\n",
       "       'split2_test_precision', 'mean_test_precision', 'std_test_precision',\n",
       "       'rank_test_precision', 'split0_train_precision',\n",
       "       'split1_train_precision', 'split2_train_precision',\n",
       "       'mean_train_precision', 'std_train_precision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = clf2.cv_results_\n",
    "results_df = pd.DataFrame.from_dict(results )\n",
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.217917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.213217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.211681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_f1\n",
       "71  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...      0.217917\n",
       "70  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...      0.213217\n",
       "67  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...      0.211681"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('mean_test_f1',ascending=False)[['params','mean_test_f1']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.843076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'learning_rate': 0.08, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.842779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'learning_rate': 0.08, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.842671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_AUC\n",
       "60  {'learning_rate': 0.12, 'max_depth': 3, 'min_s...       0.843076\n",
       "31  {'learning_rate': 0.08, 'max_depth': 3, 'min_s...       0.842779\n",
       "32  {'learning_rate': 0.08, 'max_depth': 3, 'min_s...       0.842671"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('mean_test_AUC',ascending=False)[['params','mean_test_AUC']].head(3)\n",
    "# valid score\n",
    "# why is valid AUC so different from test AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.688885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.684514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.03, 'max_depth': 3, 'min_s...</td>\n",
       "      <td>0.657128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_test_precision\n",
       "1  {'learning_rate': 0.03, 'max_depth': 3, 'min_s...             0.688885\n",
       "8  {'learning_rate': 0.03, 'max_depth': 3, 'min_s...             0.684514\n",
       "4  {'learning_rate': 0.03, 'max_depth': 3, 'min_s...             0.657128"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('mean_test_precision',ascending=False)[['params','mean_test_precision']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.169323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.166205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>{'learning_rate': 0.12, 'max_depth': 6, 'min_s...</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_recall\n",
       "71  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...          0.169323\n",
       "68  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...          0.166205\n",
       "67  {'learning_rate': 0.12, 'max_depth': 6, 'min_s...          0.163000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('mean_test_recall',ascending=False)[['params','mean_test_recall']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best params to refit the model\n",
    "gbt2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf2.predict(X_train)\n",
    "y_test_pred = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using recall\n",
    "\n",
    "using precision\n",
    "- train\n",
    "- Precision: 1.000\n",
    "- Recall: 0.006\n",
    "- F1: 0.013\n",
    "- AUC: 0.503\n",
    "- test\n",
    "- Precision: 0.000\n",
    "- Recall: 0.000\n",
    "- F1: 0.000\n",
    "- AUC: 0.500\n",
    "using f1\n",
    "- train\n",
    "- Precision: 1.000\n",
    "- Recall: 0.997\n",
    "- F1: 0.998\n",
    "- AUC: 0.998\n",
    "- test\n",
    "- Precision: 0.338\n",
    "- Recall: 0.153\n",
    "- F1: 0.211\n",
    "- AUC: 0.567\n",
    "using balanced_accuracy\n",
    "- train\n",
    "- Precision: 1.000\n",
    "- Recall: 0.997\n",
    "- F1: 0.998\n",
    "- AUC: 0.998\n",
    "- test\n",
    "- Precision: 0.323\n",
    "- Recall: 0.146\n",
    "- F1: 0.201\n",
    "- AUC: 0.564\n",
    "using average_precision\n",
    "- train\n",
    "- Precision: 0.769\n",
    "- Recall: 0.064\n",
    "- F1: 0.118\n",
    "- AUC: 0.531\n",
    "- test\n",
    "- Precision: 0.250\n",
    "- Recall: 0.014\n",
    "- F1: 0.026\n",
    "- AUC: 0.506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with refit=False\n",
    "def get_top_features(clf,X):\n",
    "    important_features = pd.Series(data=clf.feature_importances_,index=X.columns)\n",
    "    important_features.sort_values(ascending=False,inplace=True)\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_features(gbt2,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prediction(clf2, test4, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('exp4_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
